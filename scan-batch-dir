#!/usr/bin/python3
import yaml
import argparse
import os
import pwd
import json
import sys
import subprocess
import shutil
import logging
import openpyxl
import csv
import io
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build

# import from functions.py
#from functions import *

# Setup the log file format.
log_formatter = logging.Formatter(fmt='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',datefmt="%Y%m%d %H:%M:%S")


def get_username() -> str:
    """
    Retrieves the current logged-in user's username.

    Returns:
        str: The username of the current user.
    """
    return pwd.getpwuid(os.getuid())[0]

def create_temp_dir_at_path(path: str) -> str:
    """Creates a temporary directory at the specified path.

    Args:
        path: The path where the temporary directory should be created.

    Returns:
        The path to the created temporary directory.
    """
    temp_dir = tempfile.mkdtemp(dir=path)
    return temp_dir

def read_yaml_file(path: str) -> dict:
    """
    Reads a YAML file and returns its content as a dictionary.

    Args:
        path (str): The path to the YAML file.

    Returns:
        dict: The content of the YAML file.

    Raises:
        FileNotFoundError: If the file does not exist.
        yaml.YAMLError: If there is an error parsing the YAML file.
        ValueError: If the file content is not a valid dictionary.
    """
    try:
        with open(path, "r") as stream:
            data = yaml.safe_load(stream)
            if not isinstance(data, dict):
                raise ValueError(f"The file content is not a valid dictionary: {path}")
            return data
    except FileNotFoundError:
        print(f"File not found: {path}. Please check the file path and try again.")
        raise
    except yaml.YAMLError:
        print(f"Error parsing YAML file: {path}. Please ensure the file is properly formatted.")
        raise
    except ValueError:
        print(f"Invalid content in YAML file: {path}.")
        raise
    except Exception as e:
        print(f"An unexpected error occurred while reading the YAML file: {str(e)}")
        raise e

def setup_logger(name: str, log_file: str, level=logging.DEBUG) -> logging.Logger:
    """
    Sets up a logger with the specified name, log file, and logging level.

    Args:
        name (str): The name of the logger.
        log_file (str): The file to log messages to.
        level (int): The logging level (e.g., logging.DEBUG).

    Returns:
        logging.Logger: The configured logger.
    """
    handler = logging.FileHandler(log_file)
    handler.setFormatter(log_formatter)
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)
    return logger


def connect_to_google_sheet(credentials_file: str):
    """
    Connects to the Google Sheets API using service account credentials.

    Args:
        credentials_file (str): Path to the Google service account credentials file.

    Returns:
        build: The Google Sheets API service object.
    """
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
    CONFIG_FILE = credentials_file

    if not os.path.exists(CONFIG_FILE):
        raise Exception(f"Configuration file not found: {CONFIG_FILE}")

    try:
        creds = service_account.Credentials.from_service_account_file(
            CONFIG_FILE,
            scopes=SCOPES
        )

        service = build('sheets', 'v4', credentials=creds)
        return service

    except Exception as e:
        raise Exception(f"Failed to create Google Sheets service: {str(e)}")

def read_google_sheet(spreadsheet_id: str, sheet_name="Sheet1", credentials_file="configuration.json") -> pd.DataFrame:
    """
    Reads data from a Google Sheet into a pandas DataFrame.

    Args:
        spreadsheet_id (str): The ID of the Google Sheet.
        sheet_name (str): The name of the sheet to read data from.
        credentials_file (str): Path to the Google service account credentials file.

    Returns:
        pandas.DataFrame: The data from the Google Sheet.
    """
    try:
        # Build the service
        service = connect_to_google_sheet(credentials_file);

        # Call the Sheets API
        sheet = service.spreadsheets().values().get(
            spreadsheetId=spreadsheet_id,
            range=sheet_name
        ).execute()

        # Get the values
        data = sheet.get('values', [])

        if not data:
            logger.warn(f"read_google_sheet - No data found in the specified worksheet.")
            print(f"No data found in the specified worksheet.")

            # Return empty DataFrame
            return pd.DataFrame()

        else:
            logger.info(f"read_google_sheet - Read of Google Sheet Successful.")
            print(f"Read of Google Sheet Successful.")

            # Convert to DataFrame
            # First row as headers, rest as data
            headers = data[0]
            rows = data[1:] if len(data) > 1 else []

            # Pad rows with fewer columns with fill_value
            fill_value = None
            max_columns = len(headers)
            padded_rows = [row + [fill_value] * (max_columns - len(row)) for row in rows]

            # Create DataFrame
            df = pd.DataFrame(padded_rows, columns=headers)
        
            return df

    except Exception as e:
        print(f"Error accessing Google Sheet: {str(e)}")
        return None

def update_google_sheet(df: pd.DataFrame, spreadsheet_id: str, sheet_name: str, credentials_file: str) -> tuple:
    """
    Updates a Google Sheet with data from a pandas DataFrame.

    Args:
        df (pandas.DataFrame): The DataFrame containing updated data.
        spreadsheet_id (str): The ID of the Google Sheet.
        sheet_name (str): The name of the sheet to update.
        credentials_file (str): Path to the Google service account credentials file.

    Returns:
        tuple: Success status and message.
    """
    try:
        # Validate inputs
        if not isinstance(df, pd.DataFrame):
            return False, "Input must be a pandas DataFrame"

        if df.empty:
            return False, "DataFrame is empty"

        # Build the service
        service = connect_to_google_sheet(credentials_file);

        # Call the Sheets API
        sheet = service.spreadsheets()

        # Convert DataFrame to list of lists (including headers)
        values = [df.columns.tolist()] + df.values.tolist()

        # Prepare the body for the API request
        body = {
            'values': values
        }

        # Update the sheet with DataFrame contents
        result = sheet.values().update(
            spreadsheetId=spreadsheet_id,
            range=f'{sheet_name}!A1',
            valueInputOption='RAW',
            body=body
        ).execute()

        updated_cells = result.get('updatedCells', 0)
        return True, f"Successfully updated {updated_cells} cells"

    except Exception as e:
        return False, f"An error occurred: {str(e)}"

def update_dataframe(df: pd.DataFrame, match_column: str, match_value: str, update_dict):
    try:
        # Verify input is a DataFrame
        if not isinstance(df, pd.DataFrame):
            return None, False, "update_dataframe - Input must be a pandas DataFrame"

        # Check if DataFrame is empty
        if df.empty:
            return df, False, "update_dataframe - DataFrame is empty"

        # Verify columns exist
        if match_column not in df.columns:
            return df, False, f"update_dataframe - Match column '{match_column}' not found"
        #if update_column not in df.columns:
        #    return df, False, f"update_dataframe - Update column '{update_column}' not found"

        # Make a copy of the dataframe
        df_copy = df.copy()

        mask = df_copy[match_column] == match_value

        if not mask.any():
            return df, False, f"update_dataframe - No rows found matching '{match_value}' in '{match_column}'"

        for column, value in update_dict.items():
            if column in df_copy.columns:
                df_copy.loc[mask, column] = value
            else:
                logger.warning(f"update_dataframe: Column:{column} does not exist - unable to update with value: {value}.") 
                print(f"update_dataframe: Column:{column} does not exist - unable to update with value: {value}.")

        # Clean Dataframe of NaN values.
        df = df_copy.fillna('')

        return df, True, f"update_dataframe - Successfully updated dataframe row"

    except Exception as e:
        return df, False, f"update_dataframe - An error occurred: {str(e)}"
    


def add_row_to_dataframe(df, row_data, key_column):
    """
    Add a new row to a pandas DataFrame, ensuring the key column value is unique.
    
    Parameters:
    df (pandas.DataFrame): The input DataFrame
    row_data (dict): Dictionary containing the new row data with column names as keys
    key_column (str): The column name to check for uniqueness
    
    Returns:
    pandas.DataFrame: DataFrame with the new row appended if key is unique, original DataFrame otherwise
    """
    try:
        # Verify input is a DataFrame
        if not isinstance(df, pd.DataFrame):
            return None, False, "add_row_to_dataframe - Input must be a pandas DataFrame"

        # Update the Dataframe.
        # Check if key_column exists in DataFrame
        if key_column not in df.columns:
            raise ValueError(f"add_row_to_dataframe - Key column '{key_column}' not found in DataFrame")
    
        # Check if key_column exists in row_data
        if key_column not in row_data:
            raise ValueError(f"add_row_to_dataframe - Key column '{key_column}' not found in row_data")
    
        # Check if the key value already exists
        if row_data[key_column] in df[key_column].values:
            #print(f"Warning: Key value '{row_data[key_column]}' already exists in column '{key_column}'. Row not added.")
            return df, False, "add_row_to_dataframe - Row with Key column {key_column} already exists."
    
        # Convert row_data to DataFrame
        new_row = pd.DataFrame([row_data])
    
        # Append the new row to DataFrame
        df_ret = pd.concat([df, new_row], ignore_index=True)

        # Remove any NaN values.
        df = df_ret.fillna('')

        return df, True, f"add_row_to_dataframe - Successfully added dataframe row" 

    except Exception as e:
        return df, False, f"add_row_to_dataframe - Failed to add dataframe row: {str(e)}"


def value_exists_in_column(df, column_name, value):
    """
    Checks if a value exists in a column in a Pandas DataFrame.

    Args:
        df (pd.DataFrame): Pandas DataFrame to search in.
        column_name (str): Name of the column to search in.
        value: Value to search for.

    Returns:
        bool: True if the value exists, False otherwise.
    """

    try:
        # Check if the column exists in the DataFrame
        if column_name not in df.columns:
            return False
            #raise ValueError(f"The column '{column_name}' does not exist in the DataFrame")

        # Check if the value exists in the column
        return value in df[column_name].values

    except Exception as e:
        #print(f"An unexpected error occurred: {e}")
        return False

def scan_directory(directory):
    """
    Recursively scan a directory and return a list of files and subdirectories.
    
    Args:
        directory (str): Path to the directory to scan.
    
    Returns:
        list: List of tuples containing file/subdirectory name, type, and path.
    """
    result = []
    for item in os.listdir(directory):
        item_path = os.path.join(directory, item)
        if os.path.isfile(item_path):
            result.append((item, "File", item_path))
        elif os.path.isdir(item_path):
            result.append((item, "Directory", item_path))
            result.extend(scan_directory(item_path))  # Recursive call
    return result


def convert_tiff_to_jp2(tif_file_path, jp2_file_path):
    """
    Converts a tif file to a jp2 file using the 'gm convert' command.

    Args:
        tif_file_path (str): A full path the TIFF file.
        jp2_file_path (str): A full path to the resulting JP2 file.
    Returns:
        None 
    """
    args = [
        "/usr/bin/gm",
        "convert",
        "-define",
        "jp2:numrlvls=6",
        "-define",
        "jp2:tilewidth=1024",
        "-define",
        "jp2:tileheight=1024",
        "-define",
        "jp2:rate=1.0",
        "-define",
        "jp2:lazy", 
        "-define",
        "jp2:prg=rlcp",
        "-define",
        "jp2:mode=int",
        "-define", 
        "jp2:ilyrrates='0.015625,0.01858,0.0221,0.025,0.03125,0.03716,0.04419,0.05,0.0625,0.075,0.088,0.1,0.125,0.15,0.18,0.21,0.25,0.3,0.35,0.4,0.5,0.6,0.7,0.84'",
        tif_file_path,
        jp2_file_path
        ]

    try:
        result = subprocess.run(args, capture_output=True, text=True)
        if (result.returncode == 0):
            logger.info(f"Successfully converted TIFF to JP2 on file: {tif_file_path}")
    except Exception as e:
        raise Exception(f"Failed to convert TIFF to JP2: {str(e)}")

def is_valid_dataframe(df: pd.DataFrame) -> bool:
    """
    Determines if a DataFrame is valid (not empty and of correct type).

    Args:
        df (pandas.DataFrame): The DataFrame to validate.

    Returns:
        bool: True if valid, False otherwise.
    """
    return isinstance(df, pd.DataFrame) # and not df.empty


def parse_arguments():
    parser = argparse.ArgumentParser(
        description='Migrate media from Islandora 7 to PeerTube.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Required arguments
    parser.add_argument('--config-file', dest="config_file", required=True, help='Path to the YAML configuration file.')
    parser.add_argument('--log-file', dest="log_file", required=True, help='Path to the log file.')
    parser.add_argument('--directory', dest="directory", required=True, help='Path to the directory to scan.')

    # Optional arguments
    parser.add_argument('--in-google-sheet-id', dest="in_gs_id", help='Google Sheet ID related to the directory.')
    parser.add_argument('--in-google-sheet-name', dest="in_gs_name", help='Google Sheet Tab Name.')
    parser.add_argument('--in-google-creds-file', dest="in_gs_creds", help='Google Credentials json file.')
    
    # Parse Arguments
    args = parser.parse_args()

    return args

def process_tiff(file_path:str):
    # Process a .tif file.
    #print(f"Processing a TIFF file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)

    # Create the JP2 derivative if it does not already exist.
    # if the JP2 does not exist:
    jp2_path = f"{dir}/{pid}.jp2"
    out_pid  = f"{parent}-{pid}"

    logger.info(f"Tiff: Creating JP2: {jp2_path}")
    print(f"Tiff: Creating JP2: {jp2_path} from {file_path}")
    #if not os.path.exists(jp2_path):
    #    convert_tiff_to_jp2(file_path, jp2_path)

    return out_pid,jp2_path

def process_mp3(file_path:str):
    # Process a .mp3 file.
    #print(f"Processing a MP3 file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)
    out_pid = f"{parent}-{pid}"
    mp3_path = f"{dir}/{pid}{ext}"

    return out_pid,mp3_path

def process_transcript(file_path:str):
    # Process a .vtt or .srt file.
    #print(f"Processing a WebVTT/SRT file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)
    out_pid = f"{parent}-{pid}"
    vtt_path = f"{dir}/{pid}{ext}"

    return out_pid,vtt_path

def process_thumbnail(file_path:str):
    # Process a thumbnail (.jpg/.png).
    #print(f"Processing a Thumbnail file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)
    out_pid = f"{parent}-{pid}"
    tn_path = f"{dir}/{pid}{ext}"

    return out_pid,tn_path

def process_unknown(file_path:str):
    # Process a unknown file.
    #print(f"Processing an Unknown file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)
    out_pid = f"{parent}-{pid}"
    unknown_path = f"{dir}/{pid}{ext}"
    
    return out_pid,unknown_path


def get_file_info(file_path: str):
    dir,ext = os.path.splitext(file_path)   # Split the file_path into directory and extension.
    file_name = os.path.basename(dir)       # Get the file_name of the object which will usually be the pid.
    pid = file_name                         # Assign pid to the last piece of the directory.
    parent = os.path.basename(os.path.dirname(file_path))
    dir = os.path.dirname(file_path)
    file = f"{pid}{ext}"

    return(pid,parent,file,dir,ext)


def add_update_dataframe(df, pid: str, row_data, file_name: str):
    if (value_exists_in_column(df,'id',pid)):
        logger.info(f"Updating Dataframe: {pid},{file_name}")
        updated_df,success,msg = update_dataframe(df, 'id', pid, row_data)
        if not (success):
            logger.warning(f"Update DataFrame: Failed for PID: {pid} - {msg}")
            print(f"Update DataFrame: failed for PID: {pid}")
            return df
        else:
            logger.info(f"Updating DataFrame: Success for PID: {pid}")
            return updated_df
    else:
        logger.info(f"Adding to Dataframe: {pid},{file_name}")
        updated_df,success,msg = add_row_to_dataframe(df,row_data,key_column='id')
        if not (success):
            logger.warning(f"Adding to DataFrame: Failed for PID: {pid} - {msg}")
            print(f"Added to DataFrame: Failed for PID: {pid}")
            return df
        else:
            logger.info(f"Adding to DataFrame: Success for PID: {pid}")
            return updated_df


def process_object(file_type: str, file_path: str, parent: str, df):
    #logger.info(f"Processing an object.")
    # object files are kept within a folder of the top level folder.
    # files are .tif files which need to be converted.

    if not (is_valid_dataframe(df)):
        logger.warning("process_object: Incoming dataframe is invalid.")
        print(f"process_object: Incoming dataframe is invalid.")
        print(f"Invalid DataFrame: {df}")

    # Process File...
    if (file_type == "File" ) and ( parent != "" ):
        #print(f"Processing File")
        ignore_file_list = ["target.tif","manifest.csv","manifest.xlsx","manifest.xls"]

        dir,ext = os.path.splitext(file_path)
        file_name = os.path.basename(dir)
        pid = file_name

        if ( ext == ".tif" ):
            logger.info(f"Processing TIFF: {file_path}")
            print(f"Processing TIFF: {file_path}")
            outpid,outfile = process_tiff(file_path)
            row_data = {'id': outpid, 'file': outfile}
            updated_df = add_update_dataframe(df,outpid,row_data,outfile)
            return updated_df

            #if (value_exists_in_column(df,'id',outpid)):
            #    # Update existing info.
            #    logger.info(f"Updating Dataframe: {outpid},{outfile}")
            #
            #    # Set Row Data.
            #    row_data = {'id': outpid, 'file': outfile}
            #
            #    # Update the DataFrame.
            #    updated_df,success,msg = update_dataframe(df, 'id', outpid, row_data)
            #
            #    if not (success):
            #        logger.warning(f"Update DataFrame: Failed for PID: {outpid} - {msg}")
            #        print(f"Update DataFrame: failed for PID: {outpid}")
            #    else:
            #        return updated_df
            #
            #else:
            #    # Add new info.
            #    logger.info(f"Adding to Dataframe: {outpid},{outfile}")
            #
            #    # Set Row Data.
            #    row_data = {'id': outpid, 'file': outfile}
            #
            #    # Update the dataframe.
            #    updated_df,success,msg = add_row_to_dataframe(df,row_data,key_column='id')
            #
            #    if not (success):
            #        logger.warning(f"Adding to Dataframe: Failed for PID: {outpid} - {msg}")
            #        print(f"Add to DataFrame: failed for PID: {outpid}")
            #    else:
            #        return updated_df

        # Other types of files...
        elif ( ext == ".mp3" ):
            logger.info(f"Processing MP3: {file_path}")
            print(f"Procesing MP3: {file_path}")
            outpid,outfile = process_mp3(file_path)
            row_data = {'id': outpid, 'file': outfile}
            updated_df = add_update_dataframe(df,outpid,row_data,outfile)
            return updated_df

        elif (( ext == ".vtt" ) or ( ext == ".srt" )):
            logger.info(f"Processing WebVTT/SRT: {file_path}")
            print(f"Processing WebVTT/SRT: {file_path}")
            outpid,outfile = process_transcript(file_path)
            row_data = {'id': outpid, 'transcript': outfile}
            updated_df = add_update_dataframe(df,outpid,row_data,outfile) 
            return updated_df

        elif (( ext == ".jpg" ) or ( ext == ".png" )):
            logger.info(f"Processing Thumbnail: {file_path}")
            print(f"Processing Thumbnail: {file_path}")
            outpid,outfile = process_thumbnail(file_path)
            row_data = {'id': outpid, 'thumbnail': outfile}
            updated_df = add_update_dataframe(df,outpid,row_data,outfile)
            return updated_df

        else:
            logger.info(f"Processing Unknown: {file_path}")
            print(f"Unknown File extension: {ext} - Skipping.")
            outpid,outfile = process_unknown(file_path)
            row_data = {'id': outpid, 'file': outfile}
            updated_df = add_update_dataframe(df,outpid,row_data,outfile)
            return updated_df


    # Process Top Level file...
    elif (file_type == "File") and (parent == ""):
        # Top Level file.
        #print(f"Processing Top-Level File")
        ignore_file_list = ["ignore"]
        if file_path.casefold() in ignore_file_list:
            logger.info(f"Ingoring: {file_path}")
        else:
            logger.info(f"Processing Top Level file: {file_path}")
            dir,ext = os.path.splitext(file_path)
            pid = os.path.basename(dir)

            if ( ext == ".tif" ):
                # Top Level file is a .tif file.
                logger.info(f"Processing Top Level TIFF: {file_path}")
                print(f"Processing Top Level TIFF: {file_path}")
                outpid,outfile = process_tiff(file_path)
                row_data = {'id': pid, 'file': outfile}
                updated_df = add_update_dataframe(df,pid,row_data,outfile)
                return updated_df

            elif ( ext == ".mp3" ):
                logger.info(f"Processing MP3: {file_path}")
                print(f"Procesing MP3: {file_path}")
                outpid,outfile = process_mp3(file_path)
                row_data = {'id': pid, 'file': outfile}
                updated_df = add_update_dataframe(df,pid,row_data,outfile)
                return updated_df

            elif (( ext == ".vtt" ) or ( ext == ".srt" )):
                logger.info(f"Processing WebVTT/SRT: {file_path}")
                print(f"Processing WebVTT/SRT: {file_path}")
                outpid,outfile = process_transcript(file_path)
                row_data = {'id': pid, 'transcript': outfile}
                updated_df = add_update_dataframe(df,pid,row_data,outfile)
                return updated_df

            elif (( ext == ".jpg" ) or ( ext == ".png" )):
                logger.info(f"Processing Thumbnail: {file_path}")
                print(f"Processing Thumbnail: {file_path}")
                outpid,outfile = process_thumbnail(file_path)
                row_data = {'id': pid, 'thumbnail': outfile}
                updated_df = add_update_dataframe(df,pid,row_data,outfile)
                return updated_df

            else:
                logger.info(f"Processing Unknown: {file_path}")
                print(f"Unknown File extension: {ext} - Skipping.")
                outpid,outfile = process_unknown(file_path)
                row_data = {'id': pid, 'file': outfile}
                updated_df = add_update_dataframe(df,pid,row_data,outfile)
                return updated_df

    # Process Directory... 
    elif (file_type == "Directory"):
        #print(f"Processing Directory")
        ignore_dir_list = ["ignore"]
        if file_path.casefold() in ignore_dir_list:
            logger.info(f"Ignoreing: {file_path}")
        else:
            # Continue
            logger.info(f"Processing Directory: {file_path}")
            dir,ext = os.path.splitext(file_path)
            pid = os.path.basename(dir)
            
            # Check if pid in Google Sheet.
            if (value_exists_in_column(df,'id',pid)):
                # Update Existing info.
                print(f"Found: {pid} in Google Sheet")
                logger.info(f"Found: {pid} in Google Sheet")

                # Set Row Data
                row_data = {'id': pid}

                # Update the dataframe.
                logger.info(f"Updating DataFrame: {pid}")
                #updated_df,success,msg = update_dataframe(df, 'id', pid, row_data)
                df,success,msg = update_dataframe(df, 'id', pid, row_data)

                if not (success):
                    logger.warning(f"Update DataFrame: Failed for PID: {pid} - {msg}")
                    print(f"Update DataFrame: Failed for PID: {pid} - {msg}")
                else:
                    #return updated_df
                    return df

            else:
                # Add new info.
                print(f"Not Found: Adding {pid} to DataFrame")
                logger.info(f"Not Found: Adding {pid} to DataFrame")

                # Set Row Data.
                row_data = {'id': pid}

                # Update the dataframe.
                logger.info(f"Updating DataFrame: {pid}")
                print(f"Adding: {row_data}")
                #updated_df,success,msg = add_row_to_dataframe(df,row_data,key_column='id')
                df,success,msg = add_row_to_dataframe(df,row_data,'id')

                if not (success):
                    logger.warning(f"Add to DataFrame: failed for PID: {pid} - {msg}")
                    print(f"Add to DataFrame: Failed for PID: {pid} - {msg}")
                else:
                    return df
    
    else:
        print(f"Unknown object.")
        exit()


def process_objects(data,directory,df):

    #print(f"process_objects: Input DataFrame:")
    #print(df)

    # Begin Processing the listing of the directory.
    logger.info(f"Processing file data.")
    print(f"Processing the directory: {directory}")

    # Loop through the listing.
    for row, (file_name, file_type, file_path) in enumerate(data, start=2):
        # Skip things we don't want to include.
        if (( file_name == "target.tif" ) or ( file_name == "manifest.csv" ) or ( file_name == "manifest.xlsx" )):
            next
        else:
            print(f"Inspecting: {file_path}")
            # Define the parent
            parent = file_path.replace(directory, "")
            parent = parent.replace("/"+file_name, "")
            parent = parent.replace("/","")

            #print(f"File_type: {file_type}")
            #print(f"File_Path: {file_path}")
            #print(f"Parent:    {parent}")

            df = process_object(file_type,file_path,parent,df)

        print(f"\n")

    # Display the DataFrame before sending it to Google Sheets.
    #print(f"Result Dataframe:")
    #print(df)

    # Save the DataFrame to Google Sheets.
    logger.info(f"Updating Google Sheet with DataFrame.")
    print(f"Updating Google Sheet with DataFrame.")
    success,msg = update_google_sheet(df, google_sheet_id, google_sheet_name, google_credentials)
    if (success):
        logger.info(f"Successfully Updated Google Sheet.")
        print(f"Successfully Updated Google Sheet.")
    else:
        logger.warning(f"Failed to update Google Sheet: {msg}")
        print(f"Failed to update Google Sheet: {msg}")


def main():
    # Setup the log file format.
    globals()['log_formatter'] = logging.Formatter(fmt='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',datefmt="%Y%m%d %H:%M:%S")

    # Parse command line arguements.
    globals()['args'] = parse_arguments()

    # Obtain the user running the script.
    username            = get_username()

    # Get the configuration file contents.
    globals()['cfg'] = read_yaml_file(args.config_file)

    # Set Google variables from config file.
    globals()['google_credentials']  = cfg['google_credentials_file']
    globals()['google_sheet_id']     = cfg['google_sheet_id']
    globals()['google_sheet_name']   = cfg['google_sheet_name']
    globals()['log_file']            = cfg['log_file']

    # Override config file variables with command line parameters.
    if args.in_gs_creds is not None:
        globals()['google_credentials'] = args.in_gs_creds
    if args.in_gs_id is not None:
        globals()['google_sheet_id'] = args.in_gs_id
    if args.in_gs_name is not None: 
        globals()['google_sheet_name'] = args.in_gs_name
    if args.log_file is not None:
        globals()['log_file'] = args.log_file

    # Create the Log file.
    #print(f"Creating log file: {args.log_file}")
    #globals()['logger'] = setup_logger('logger', args.log_file, level=logging.DEBUG)
    globals()['logger'] = setup_logger('logger', log_file, level=logging.DEBUG)
    logger.info(f"Begin log.")

    # Get external command paths.
    gm_path = shutil.which("gm")

    # Check 'gm' exists.
    if gm_path:
        logger.info(f"GraphicsMagick Executable found at: {gm_path}")
    else:
        logger.error(f"GraphicsMagick Executable 'gm' not found and is required.")
        print(f"GraphicsMagick Executable 'gm' not found and is required.")
        print(f"Exiting...")
        exit()

    # Connect to Google Sheets and read the Sheet to dataframe.
    logger.info(f"Reading Google Sheet: {google_sheet_id},{google_sheet_name}")
    print(f"Reading Google Sheet: {google_sheet_id},{google_sheet_name}")
    df = read_google_sheet(google_sheet_id, google_sheet_name, google_credentials)

    # Scan the directory and return a list of directory contents.
    logger.info(f"Scan the directory: {args.directory}")
    file_data = scan_directory(args.directory)

    # Process the contents.
    logger.info(f"Process the directory: {args.directory}")
    process_objects(file_data,args.directory,df)

    exit()


# Setup global variables.
google_credentials = None
google_sheet_id = None
google_sheet_name = None

if __name__ == "__main__":
    main()


