#!/usr/bin/python3
import yaml
import argparse
import os
import pwd
import json
import sys
import subprocess
import shutil
import logging
import openpyxl
import csv
import io
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build

# import from functions.py
#from functions import *

# Setup the log file format.
log_formatter = logging.Formatter(fmt='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',datefmt="%Y%m%d %H:%M:%S")


def get_username() -> str:
    """
    Retrieves the current logged-in user's username.

    Returns:
        str: The username of the current user.
    """
    return pwd.getpwuid(os.getuid())[0]

def create_temp_dir_at_path(path: str) -> str:
    """Creates a temporary directory at the specified path.

    Args:
        path: The path where the temporary directory should be created.

    Returns:
        The path to the created temporary directory.
    """
    temp_dir = tempfile.mkdtemp(dir=path)
    return temp_dir

def read_yaml_file(path: str) -> dict:
    """
    Reads a YAML file and returns its content as a dictionary.

    Args:
        path (str): The path to the YAML file.

    Returns:
        dict: The content of the YAML file.

    Raises:
        FileNotFoundError: If the file does not exist.
        yaml.YAMLError: If there is an error parsing the YAML file.
        ValueError: If the file content is not a valid dictionary.
    """
    try:
        with open(path, "r") as stream:
            data = yaml.safe_load(stream)
            if not isinstance(data, dict):
                raise ValueError(f"The file content is not a valid dictionary: {path}")
            return data
    except FileNotFoundError:
        print(f"File not found: {path}. Please check the file path and try again.")
        raise
    except yaml.YAMLError:
        print(f"Error parsing YAML file: {path}. Please ensure the file is properly formatted.")
        raise
    except ValueError:
        print(f"Invalid content in YAML file: {path}.")
        raise
    except Exception as e:
        print(f"An unexpected error occurred while reading the YAML file: {str(e)}")
        raise e

def setup_logger(name: str, log_file: str, level=logging.DEBUG) -> logging.Logger:
    """
    Sets up a logger with the specified name, log file, and logging level.

    Args:
        name (str): The name of the logger.
        log_file (str): The file to log messages to.
        level (int): The logging level (e.g., logging.DEBUG).

    Returns:
        logging.Logger: The configured logger.
    """
    handler = logging.FileHandler(log_file)
    handler.setFormatter(log_formatter)
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)
    return logger


def connect_to_google_sheet(credentials_file: str):
    """
    Connects to the Google Sheets API using service account credentials.

    Args:
        credentials_file (str): Path to the Google service account credentials file.

    Returns:
        build: The Google Sheets API service object.
    """
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
    CONFIG_FILE = credentials_file

    if not os.path.exists(CONFIG_FILE):
        raise Exception(f"Configuration file not found: {CONFIG_FILE}")

    try:
        creds = service_account.Credentials.from_service_account_file(
            CONFIG_FILE,
            scopes=SCOPES
        )

        service = build('sheets', 'v4', credentials=creds)
        return service

    except Exception as e:
        raise Exception(f"Failed to create Google Sheets service: {str(e)}")

def read_google_sheet(spreadsheet_id: str, sheet_name="Sheet1", credentials_file="configuration.json") -> pd.DataFrame:
    """
    Reads data from a Google Sheet into a pandas DataFrame.

    Args:
        spreadsheet_id (str): The ID of the Google Sheet.
        sheet_name (str): The name of the sheet to read data from.
        credentials_file (str): Path to the Google service account credentials file.

    Returns:
        pandas.DataFrame: The data from the Google Sheet.
    """
    try:
        # Build the service
        service = connect_to_google_sheet(credentials_file);

        # Call the Sheets API
        sheet = service.spreadsheets().values().get(
            spreadsheetId=spreadsheet_id,
            range=sheet_name
        ).execute()

        # Get the values
        data = sheet.get('values', [])

        if not data:
            print('No data found in the specified worksheet.')
            # Return empty DataFrame
            return pd.DataFrame()

        # Convert to DataFrame
        # First row as headers, rest as data
        headers = data[0]
        rows = data[1:] if len(data) > 1 else []

        # Pad rows with fewer columns with fill_value
        fill_value = None
        max_columns = len(headers)
        padded_rows = [row + [fill_value] * (max_columns - len(row)) for row in rows]

        # Create DataFrame
        df = pd.DataFrame(padded_rows, columns=headers)
        
        return df

    except Exception as e:
        print(f"Error accessing Google Sheet: {str(e)}")
        return None

def update_google_sheet(df: pd.DataFrame, spreadsheet_id: str, sheet_name: str, credentials_file: str) -> tuple:
    """
    Updates a Google Sheet with data from a pandas DataFrame.

    Args:
        df (pandas.DataFrame): The DataFrame containing updated data.
        spreadsheet_id (str): The ID of the Google Sheet.
        sheet_name (str): The name of the sheet to update.
        credentials_file (str): Path to the Google service account credentials file.

    Returns:
        tuple: Success status and message.
    """
    try:
        # Validate inputs
        if not isinstance(df, pd.DataFrame):
            return False, "Input must be a pandas DataFrame"

        if df.empty:
            return False, "DataFrame is empty"

        # Build the service
        service = connect_to_google_sheet(credentials_file);

        # Call the Sheets API
        sheet = service.spreadsheets()

        # Convert DataFrame to list of lists (including headers)
        values = [df.columns.tolist()] + df.values.tolist()

        # Prepare the body for the API request
        body = {
            'values': values
        }

        # Update the sheet with DataFrame contents
        result = sheet.values().update(
            spreadsheetId=spreadsheet_id,
            range=f'{sheet_name}!A1',
            valueInputOption='RAW',
            body=body
        ).execute()

        updated_cells = result.get('updatedCells', 0)
        return True, f"Successfully updated {updated_cells} cells"

    except Exception as e:
        return False, f"An error occurred: {str(e)}"

def update_dataframe(df: pd.DataFrame, match_column: str, match_value, update_column: str, update_value):
    """
    Update a DataFrame by matching a value in a specified column and updating another column.

    Parameters:
    df (pandas.DataFrame): The input DataFrame
    match_column (str): Name of the column to match on
    match_value: Value to match in the match_column
    update_column (str): Name of the column to update
    update_value: Value to set in the update_column

    Returns:
    pandas.DataFrame: Updated DataFrame
    boolean: True/False
    str: Message
    """
    try:
        # Verify input is a DataFrame
        if not isinstance(df, pd.DataFrame):
            return None, False, "Input must be a pandas DataFrame"

        # Check if DataFrame is empty
        if df.empty:
            return df, False, "DataFrame is empty"

        # Verify columns exist
        if match_column not in df.columns:
            return df, False, f"Match column '{match_column}' not found"
        if update_column not in df.columns:
            return df, False, f"Update column '{update_column}' not found"

        # Create a copy to avoid modifying the original
        updated_df = df.copy()

        # Find rows to update
        mask = updated_df[match_column].astype(str) == str(match_value)
        if not mask.any():
            return updated_df, False, f"No rows found matching '{match_value}' in '{match_column}'"

        # Update the matching rows
        updated_df.loc[mask, update_column] = update_value

        # Count updated rows
        updated_count = mask.sum()

        return updated_df, True, f"Successfully updated {updated_count} row(s)"

    except Exception as e:
        return df, False, f"An error occurred: {str(e)}"

def add_row_to_dataframe(df, row_data, key_column):
    """
    Add a new row to a pandas DataFrame, ensuring the key column value is unique.
    
    Parameters:
    df (pandas.DataFrame): The input DataFrame
    row_data (dict): Dictionary containing the new row data with column names as keys
    key_column (str): The column name to check for uniqueness
    
    Returns:
    pandas.DataFrame: DataFrame with the new row appended if key is unique, original DataFrame otherwise
    """
  
    # Get existing Google Sheet DataFrame.
    current_df = read_google_sheet(google_sheet_id, google_sheet_name, google_credentials)

    #
    current_df.columns = ['id', 'title', 'file']

    # Update the Dataframe.
    # Check if key_column exists in DataFrame
    if key_column not in current_df.columns:
        raise ValueError(f"Key column '{key_column}' not found in DataFrame")
    
    # Check if key_column exists in row_data
    if key_column not in row_data:
        raise ValueError(f"Key column '{key_column}' not found in row_data")
    
    # Check if the key value already exists
    if row_data[key_column] in current_df[key_column].values:
        print(f"Warning: Key value '{row_data[key_column]}' already exists in column '{key_column}'. Row not added.")
        return df
    
    # Convert row_data to DataFrame
    new_row = pd.DataFrame([row_data])
    
    # Append the new row to DataFrame
    return pd.concat([current_df, new_row], ignore_index=True)

def value_exists_in_column(df, column_name, value):
    """
    Checks if a value exists in a column in a Pandas DataFrame.

    Args:
        df (pd.DataFrame): Pandas DataFrame to search in.
        column_name (str): Name of the column to search in.
        value: Value to search for.

    Returns:
        bool: True if the value exists, False otherwise.
    """

    try:
        # Check if the column exists in the DataFrame
        if column_name not in df.columns:
            return False
            #raise ValueError(f"The column '{column_name}' does not exist in the DataFrame")

        # Check if the value exists in the column
        return value in df[column_name].values

    except Exception as e:
        #print(f"An unexpected error occurred: {e}")
        return False

def scan_directory(directory):
    """
    Recursively scan a directory and return a list of files and subdirectories.
    
    Args:
        directory (str): Path to the directory to scan.
    
    Returns:
        list: List of tuples containing file/subdirectory name, type, and path.
    """
    result = []
    for item in os.listdir(directory):
        item_path = os.path.join(directory, item)
        if os.path.isfile(item_path):
            result.append((item, "File", item_path))
        elif os.path.isdir(item_path):
            result.append((item, "Directory", item_path))
            result.extend(scan_directory(item_path))  # Recursive call
    return result


def read_csv(csv_file):
    """
    Read a csv file.

    Args:
        csv_file (str): Path to a CSV file.

    Returns:
        data frame (df): A Pandas data frame.
    """
    try:
        df = pd.read_csv(csv_file)
    except FileNotFoundError:
        print(f"Error: The file '{csv_file}' was not found.")
    except pd.errors.EmptyDataError:
        print(f"Error: The file '{csv_file}' is empty.")
    except pd.errors.ParserError as e:
        print(f"Error: An error occurred while parsing the file '{csv_file}' - {e}.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    return df


def write_csv(df,csv_file):
    """
    Write a csv file.

    Args:
        df (Pandas data frame): Dataframe containing the data to write out.
        csv_file (str): A full path to the resulting CSV file.
    """
    try:
        df.to_csv(csv_file, index=False)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

def update_csv(df,update_column,condition_column,condition_value,new_value):
    """
    Updates a column field in a pandas dataframe based on another column field.

    Args:
        df (pandas dataframe): pandas dataframe
        update_column (str): Name of the column to update.
        condition_column (str): Name of the column to check the condition
        condition_value (str): Value to check in the condition column.
        new_value (str): New value to update in the update column.

    Returns:
        None
    """

    #print(f"Updating: {condition_column}:{condition_value} field {update_column} with {new_value}")

    try:
        if update_column not in df.columns or condition_column not in df.columns:
            print(f"Error: {update_column} does not exist in the CSV.")
            #raise ValueError("Invalid column name")

        if condition_value not in df[condition_column].values:
            print(f"Error: {condition_column}:{condition_value} does not exist in CSV.")
            #raise ValueError("Condition value not found is not in the manifest.csv")

        df.loc[df[condition_column] == condition_value, update_column] = new_value

    except ValueError as e:
        print(f"Error: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    

def convert_tiff_to_jp2(tif_file_path, jp2_file_path):
    """
    Converts a tif file to a jp2 file using the 'gm convert' command.

    Args:
        tif_file_path (str): A full path the TIFF file.
        jp2_file_path (str): A full path to the resulting JP2 file.
    Returns:
        None 
    """
    args = [
        "/usr/bin/gm",
        "convert",
        "-define",
        "jp2:numrlvls=6",
        "-define",
        "jp2:tilewidth=1024",
        "-define",
        "jp2:tileheight=1024",
        "-define",
        "jp2:rate=1.0",
        "-define",
        "jp2:lazy", 
        "-define",
        "jp2:prg=rlcp",
        "-define",
        "jp2:mode=int",
        "-define", 
        "jp2:ilyrrates='0.015625,0.01858,0.0221,0.025,0.03125,0.03716,0.04419,0.05,0.0625,0.075,0.088,0.1,0.125,0.15,0.18,0.21,0.25,0.3,0.35,0.4,0.5,0.6,0.7,0.84'",
        tif_file_path,
        jp2_file_path
        ]

    try:
        result = subprocess.run(args, capture_output=True, text=True)
        if (result.returncode == 0):
            logger.info(f"Successfully converted TIFF to JP2 on file: {tif_file_path}")
    except Exception as e:
        raise Exception(f"Failed to convert TIFF to JP2: {str(e)}")

def is_valid_dataframe(df: pd.DataFrame) -> bool:
    """
    Determines if a DataFrame is valid (not empty and of correct type).

    Args:
        df (pandas.DataFrame): The DataFrame to validate.

    Returns:
        bool: True if valid, False otherwise.
    """
    return isinstance(df, pd.DataFrame) # and not df.empty


def validate_spreadsheet(csv_file):
    required_columns = ['id','title','file','parent_id']

    # Read the CSV file.
    df = pd.read_csv(csv_file)

    # Check for the columns.
    for column_name in required_columns:
        if not column_name in df.columns:
            print(f"Required column {column_name} is missing in the CSV {csv_file} - adding it: {column_name}.")
            # add the required column
            if column_name == 'file':
                df[column_name] = None

    # Write the CSV file.
    #write_csv(df,csv_file) 

def parse_arguments():
    parser = argparse.ArgumentParser(
        description='Migrate media from Islandora 7 to PeerTube.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Required arguments
    parser.add_argument('--config-file', dest="config_file", required=True, help='Path to the YAML configuration file.')
    parser.add_argument('--log-file', dest="log_file", required=True, help='Path to the log file.')
    parser.add_argument('--directory', dest="directory", required=True, help='Path to the directory to scan.')

    # Optional arguments
    parser.add_argument('--in-google-sheet-id', dest="in_gs_id", help='Google Sheet ID related to the directory.')
    parser.add_argument('--in-google-sheet-name', dest="in_gs_name", help='Google Sheet Tab Name.')
    
    # Parse Arguments
    args = parser.parse_args()

    return args

def process_tiff(file_path:str):
    # Process a .tif file.
    print(f"Processing a TIFF file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)

    # Create the JP2 derivative if it does not already exist.
    # if the JP2 does not exist:
    jp2_path = f"{dir}/{pid}.jp2"
    out_pid  = f"{parent}-{pid}"

    logger.info(f"Tiff: Creating JP2: {jp2_path}")
    print(f"Tiff: Creating JP2: {jp2_path} from {file_path}")
    #if not os.path.exists(jp2_path):
    #    convert_tiff_to_jp2(file_path, jp2_path)

    return out_pid,jp2_path

def process_mp3(file_path:str):
    # Process a .mp3 file.
    print(f"Processing a MP3 file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)
    out_pid = f"{parent}-{pid}"

    mp3_path = f"{dir}/{pid}{ext}"

    return out_pid,mp3_path

def process_vtt(file_path:str):
    # Process a .vtt file.
    print(f"Processing a WebVTT file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)
    out_pid = f"{parent}-{pid}"
    vtt_path = f"{dir}/{pid}{ext}"

    return out_pid,vtt_path

def process_srt(file_path:str):
    # Process a .srt file.
    print(f"Processing a SRT file: {file_path}.")
    pid,parent,file,dir,ext = get_file_info(file_path)
    out_pid = f"{parent}-{pid}"
    srt_path = f"{dir}/{pid}{ext}"

    return out_pid,srt_path


def get_file_info(file_path: str):
    dir,ext = os.path.splitext(file_path)   # Split the file_path into directory and extension.
    file_name = os.path.basename(dir)       # Get the file_name of the object which will usually be the pid.
    pid = file_name                         # Assign pid to the last piece of the directory.
    parent = os.path.basename(os.path.dirname(file_path))
    dir = os.path.dirname(file_path)
    file = f"{pid}{ext}"

    return(pid,parent,file,dir,ext)


def process_object(file_type: str, file_path: str, parent: str, df):
    #logger.info(f"Processing an object.")
    # object files are kept within a folder of the top level folder.
    # files are .tif files which need to be converted.

    if (file_type == "File" ) and ( parent != "" ):
        dir,ext = os.path.splitext(file_path)
        file_name = os.path.basename(dir)
        pid = file_name
        obj_pid = f"{parent}-{file_name}"

        if ( ext == ".tif" ):
            logger.info(f"Processing TIFF: {file_path}")
            print(f"Processing TIFF: {file_path}")
            outpid,outfile = process_tiff(file_path)

            if (value_exists_in_column(df,'id',pid)):
                # Update the sheet.
                logger.info(f"Updating Dataframe: {outpid},{outfile}")
                #updated_df,success,msg = update_dataframe(df, match_column, match_value, update_column, update_value)
                updated_df,success,msg = update_dataframe(df, 'id', outpid, 'file', outfile)

                # Write the changes to the google sheet.
                success,msg = update_google_sheet(updated_df, google_sheet_id, google_sheet_name, google_credentials)
            else:
                # Add it to the sheet.
                row_data = {'id': outpid, 'title':'', 'file': outfile}

                # Update the dataframe.
                logger.info(f"Updating the DataFrame with new {pid}")
                updated_df = add_row_to_dataframe(df,row_data,key_column='id')

                # Write the changes to the google sheet.
                success,msg = update_google_sheet(updated_df, google_sheet_id, google_sheet_name, google_credentials)

        if ( ext == ".mp3" ):
            logger.info(f"Processing MP3: {file_path}")
            print(f"Procesing MP3: {file_path}")
            process_mp3(file_path)
            # More to do here.
    
        if ( ext == ".vtt" ):
            logger.info(f"Processing WebVTT: {file_path}")
            print(f"Processing WebVTT: {file_path}")
            process_vtt(file_path)
            # More to do here.

        if ( ext == ".srt" ):
            print(f"Processing SRT.")
            process_srt(file_path)
            # More to do here.



def process_directory(file_type: str, file_path: str, parent: str, df):
    # folders under the top level folder are considered object folders.
    print(f"Processing Directory: {file_path}.")
    logger.info(f"Processing Directory: {file_path}.")
    
    # ignore the ignore directory:
    substring = "ignore"
    if substring.casefold() in file_path:
        logger.info(f"Ignoreing: {file_path}")
    else:
        if (file_type == "Directory"):
            dir,ext = os.path.splitext(file_path)
            dir_name = os.path.basename(dir)
            pid = dir_name

            # Check to see if the directory (object) exists in the dataframe.
            # if it does...
            if (value_exists_in_column(df,'id',pid)):
                # Found - Update the sheet 
                print(f"Found: {pid} in Google Sheet")
                logger.info(f"Found: {pid} in Google Sheet")

                # Update the dataframe.
                logger.info(f"Updating DataFrame")
                updated_df,success,msg = update_dataframe(df, 'id', pid, 'file', file_path)

                # Update the Google Sheet.
                logger.info(f"Updating Google Sheet")
                success,msg = update_google_sheet(updated_df, google_sheet_id, google_sheet_name, google_credentials)
                if (success):
                    logger.info(f"Updated Google Sheet.")
                    print(f"Updated Google Sheet")
                else:
                    logger.warn(f"Failed to update Google Sheet: {msg}")
                    print(f"Failed to update Google Sheet: {msg}")
            else:
                print(f"Not Found: Adding {pid} to Google Sheet")
                logger.info(f"Not Found: Adding {pid} to Google Sheet")

                # if it doesn't...
                # Add it to the sheet.
                row_data = {'id': pid, 'title':'', 'file': file_path}

                # Update the dataframe.
                logger.info(f"Updating DataFrame")
                updated_df = add_row_to_dataframe(df,row_data,key_column='id')

                # Update the Google Sheet.
                logger.info(f"Updating Google Sheet.")
                success,msg = update_google_sheet(updated_df, google_sheet_id, google_sheet_name, google_credentials)
                if (success):
                    logger.info(f"Updated Google Sheet.")
                    print(f"Updated Google Sheet")
                else:
                    logger.warn(f"Failed to update Google Sheet: {msg}")
                    print(f"Failed to update Google Sheet: {msg}") 

    logger.info(f"Processing Directory: {dir} - complete.")


def process_files(data,directory,df):
    # Begin Processing the listing of the directory.
    logger.info(f"Processing file data.")

    # Loop through the listing.
    for row, (file_name, file_type, file_path) in enumerate(data, start=2):
        # Skip things we don't want to include.
        if (( file_name == "target.tif" ) or ( file_name == "manifest.csv" ) or ( file_name == "manifest.xlsx" )):
            next
        else:
            print(f"Inspecting: {file_path}")
            # Define the parent
            parent = file_path.replace(directory, "")
            parent = parent.replace("/"+file_name, "")
            parent = parent.replace("/","")

            if ( file_type == "File" ):
                # This is a file
                print(f"File: {file_path}")
                process_object(file_type,file_path,parent,df)

            if (file_type == "Directory" ):
                # This is a Directory.
                print(f"Directory: {file_path}")
                process_directory(file_type,file_path,parent,df)
        print(f"\n")
    

def main():
    # Setup the log file format.
    globals()['log_formatter'] = logging.Formatter(fmt='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',datefmt="%Y%m%d %H:%M:%S")

    # Parse command line arguements.
    globals()['args'] = parse_arguments()

    # Obtain the user running the script.
    username            = get_username()

    # Get the configuration file contents.
    cfg                 = read_yaml_file(args.config_file)

    # Set Google variables from config file.
    globals()['google_credentials']  = cfg['google_credentials_file']
    globals()['google_sheet_id']     = cfg['google_sheet_id']
    globals()['google_sheet_name']   = cfg['google_sheet_name']

    # Create the Log file.
    #print(f"Creating log file: {args.log_file}")
    globals()['logger'] = setup_logger('logger', args.log_file, level=logging.DEBUG)
    logger.info(f"Begin log.")

    # Get external command paths.
    gm_path = shutil.which("gm")

    # Check 'gm' exists.
    if gm_path:
        logger.info(f"GraphicsMagick Executable found at: {gm_path}")
    else:
        logger.error(f"GraphicsMagick Executable 'gm' not found and is required.")
        print(f"GraphicsMagick Executable 'gm' not found and is required.")
        print(f"Exiting...")
        exit()

    # Connect to Google Sheets and read the Sheet to dataframe.
    logger.info(f"Reading Google Sheet: {google_sheet_id},{google_sheet_name}")
    print(f"Reading Google Sheet: {google_sheet_id},{google_sheet_name}")
    df = read_google_sheet(google_sheet_id, google_sheet_name, google_credentials)

    # Scan the directory and return a list of directory contents.
    logger.info(f"Scan the directory: {args.directory}")
    file_data = scan_directory(args.directory)

    # Process the contents.
    logger.info(f"Process the directory: {args.directory}")
    process_files(file_data,args.directory,df)

    exit()


# Define a global dataframe.
df = pd.DataFrame()

# Setup global variables.
google_credentials = None
google_sheet_id = None
google_sheet_name = None

if __name__ == "__main__":
    main()


